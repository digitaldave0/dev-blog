# Thoughts on AI Today â€“ Why Persistence Matters

**By Dave Hibbitts | June 2025**

Artificial Intelligence has come a long way in a short time. Weâ€™ve seen models generate text, code, images, and even simulate conversation. But as impressive as these capabilities are, I believe weâ€™re still missing something fundamental on the path to true intelligence.

Too much of todayâ€™s AI thinking is driven by resets.

Most large models are trained, fine-tuned, then deployed â€” and when the next version comes, we do it all over again. Even during training, vast amounts of edge-case data are removed to keep things â€œclean.â€ And once deployed, models forget everything they experience unless itâ€™s explicitly fed back into the system.

This isnâ€™t how humans learn.

We accumulate. We adapt. We remember the rare events, the mistakes, the edge cases. Our knowledge grows over time. Intelligence, in any meaningful form, isnâ€™t episodic â€” itâ€™s persistent.

---

## What AI Needs Next

- **Memory that persists across sessions**  
- **Learning architectures that evolve over time**  
- **Access to diverse, even messy, data**  
- **A shift away from optimization-only thinking**  

Scaling laws (Kaplan et al., 2020) have taught us that bigger models and more data work â€” but there are limits. The next breakthroughs in AI will come not from size alone, but from **structural changes in how systems retain and build on knowledge**.

---

## Why Founder-Model Reuse Isnâ€™t Enough

While reusing base (founder) models like GPT, LLaMA, and Claude gives us a starting point, itâ€™s also becoming a crutch. Repeating and fine-tuning on the same underlying weights leads to diminishing returns. These models are optimized for general performance â€” not for chaos, conflict, anomaly, or nuance.

I believe **true AGI will not emerge from optimizing past success**. It will emerge from learning through difference, failure, contradiction, and randomness.

We need to start thinking of **chaos as a feature** of intelligence â€” not a bug.

---

## Learning Brick by Brick

Iâ€™m currently studying foundation model architecture from the ground up â€” brick by brick. I believe we need to understand not just how these systems behave, but **how theyâ€™re built**.

This process is helping me think critically about how we train, what we keep, and what we throw away. And itâ€™s becoming clear: **weâ€™re still throwing away too much** to achive zero one shot, responses.

---

## Final Thought

AI today is powerful â€” but still narrow. To move forward, we must treat intelligence not just as computation, but as something that grows. Like a mind.

Letâ€™s stop resetting.  
Letâ€™s start remembering.  
Letâ€™s embrace chaos.  

In my humble opinion, this may sound unconventional â€” but think about how a toddler learns. They donâ€™t just observe. They touch, taste, feel. They explore the world through mess, sensation, and trial.

Current models donâ€™t do that. They see data patterns and adjust weights â€” but without experience.  

For example: does a foundation model really understand the **sea**? Or a **beach**?

It can recognize a photo. It can predict text about â€œwaves crashingâ€ or â€œsand underfoot.â€ But these are abstractions, not experiences. The model builds a probabilistic illusion â€” a *delusion model* â€” based on data correlations, not sensory reality.

Whatâ€™s missing is the chaos:  
- The roar of surf in the ears  
- The salt sting on lips  
- The unpredictability of motion and sound and texture  

These are millions of additional parameters that no current model truly engages with.

> *In cognitive science, this idea aligns with embodied cognition: that real intelligence isnâ€™t learned from labels â€” itâ€™s learned from the body interacting with the world.*

Until AI can grapple with this messy, embodied input â€” not just vision and language â€” its intelligence will remain confined.

---

## ðŸ“š References & Further Reading

- **Scaling Laws for Neural Language Models** â€“ Kaplan et al., 2020  
  [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)

- **Memory Consolidation Theory** â€“ McClelland et al., 1995  
  [https://pubmed.ncbi.nlm.nih.gov/7821215/](https://pubmed.ncbi.nlm.nih.gov/7821215/)

- **Spacing Effect and Long-Term Retention** â€“ Cepeda et al., 2006  
  [https://journals.sagepub.com/doi/10.1111/j.1467-8721.2006.00476.x](https://journals.sagepub.com/doi/10.1111/j.1467-8721.2006.00476.x)

- **The Limits of Fine-Tuning** â€“ Schaeffer et al., 2023  
  [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)

- **Why Noisy Data Might Be Better** â€“ Marin et al., 2022  
  [https://aclanthology.org/2022.findings-emnlp.12/](https://aclanthology.org/2022.findings-emnlp.12/)

---